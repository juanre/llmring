"""Command-line interface for llmring."""

import argparse
import asyncio
import json
import os
import secrets
import sys
from pathlib import Path
from typing import Optional, Tuple

import httpx
from dotenv import load_dotenv

from llmring import LLMRequest, LLMRing, Message
from llmring.cli_utils import format_error, format_info, format_success, format_warning
from llmring.constants import LOCKFILE_NAME
from llmring.lockfile_core import Lockfile
from llmring.registry import RegistryClient
from llmring.server_client import ServerClient
from llmring.service import DEFAULT_SAAS_URL

# Alias sync removed per source-of-truth v3.8 - aliases are purely local

# Load environment variables from .env file
load_dotenv()

_TRUTHY_ENV_VALUES = {"1", "true", "yes", "on"}


def _env_flag(name: str) -> bool:
    """Return True when the environment variable represents a truthy value."""
    value = os.getenv(name)
    if value is None:
        return False
    return value.strip().lower() in _TRUTHY_ENV_VALUES


def _resolve_server_settings(
    server_override: Optional[str] = None,
    api_key_override: Optional[str] = None,
    enable_saas_fallback: bool = True,
) -> Tuple[Optional[str], Optional[str], bool]:
    """Resolve server URL and API key using CLI overrides and environment."""
    env_server = os.getenv("LLMRING_SERVER_URL")
    env_key = os.getenv("LLMRING_API_KEY")
    prefer_saas = _env_flag("LLMRING_PREFER_SAAS")

    server_url = server_override if server_override is not None else env_server
    api_key = api_key_override if api_key_override is not None else env_key

    used_saas = False
    if not server_url and enable_saas_fallback and prefer_saas:
        server_url = DEFAULT_SAAS_URL
        used_saas = True

    if server_url:
        server_url = server_url.rstrip("/")

    return server_url, api_key, used_saas


def _write_env_file(path: Path, server_url: str, api_key: str, overwrite: bool = False) -> None:
    """Write environment exports for server configuration."""
    if path.exists() and not overwrite:
        raise FileExistsError(f"{path} already exists. Use --force to overwrite.")

    lines = [
        "# Generated by `llmring server init`",
        f"export LLMRING_SERVER_URL={server_url}",
        f"export LLMRING_API_KEY={api_key}",
        "",
    ]
    path.write_text("\n".join(lines), encoding="utf-8")


def _generate_api_key() -> str:
    """Generate a random API key suitable for llmring-server self-hosting."""
    return f"llmr_pk_{secrets.token_hex(16)}"


def _load_env_file(path: Path) -> dict:
    """Load simple KEY=VALUE lines from an env file."""
    if not path.exists():
        return {}
    data: dict[str, str] = {}
    for raw_line in path.read_text(encoding="utf-8").splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        if line.startswith("export "):
            line = line[7:].lstrip()
        key, value = line.split("=", 1)
        data[key.strip()] = value.strip()
    return data


def _mask_api_key(api_key: Optional[str]) -> str:
    """Render an API key with limited exposure."""
    if not api_key:
        return "(not configured)"
    if len(api_key) <= 8:
        return "*" * len(api_key)
    return f"{api_key[:4]}‚Ä¶{api_key[-4:]}"


async def cmd_lock_init(args):
    """Initialize a new lockfile with basic defaults from registry."""
    # Find package directory if not explicitly specified
    if args.file:
        path = Path(args.file)
        package_dir = path.parent
    else:
        # Try to find package directory where lockfile should be placed
        package_dir = Lockfile.find_package_directory()
        if package_dir:
            path = package_dir / LOCKFILE_NAME
            print(f"Found package directory: {package_dir}")
        else:
            # Fall back to current directory
            path = Path(LOCKFILE_NAME)
            package_dir = Path.cwd()
            print("No package directory found")
            print(f"Creating lockfile in current directory: {path.resolve()}")

    if path.exists() and not args.force:
        print(format_error(f"{path} already exists. Use --force to overwrite"))
        return 1

    print("Creating lockfile with registry-based defaults...")
    print()

    # Try to create with registry data
    try:
        from llmring.registry import RegistryClient

        registry_client = RegistryClient()
        lockfile = await Lockfile.create_default_async(registry_client)
        print("‚úÖ Created lockfile with registry data")
    except Exception as e:
        # Fallback to basic if registry unavailable
        print(f"‚ö†Ô∏è  Could not fetch registry data: {e}")
        print("   Creating minimal lockfile")
        lockfile = Lockfile.create_default()

    lockfile.save(path)

    print(f"‚úÖ Created lockfile: {path}")

    # Show default bindings
    default_profile = lockfile.get_profile("default")
    if default_profile.bindings:
        print("\nDefault aliases:")
        for binding in default_profile.bindings:
            print(f"  {binding.alias} ‚Üí {binding.model_ref}")
    else:
        print("\nNo default aliases configured.")

    # Check if we're in a Python project and provide packaging guidance
    pyproject_path = package_dir / "pyproject.toml"
    if pyproject_path.exists():
        print("\n‚ö†Ô∏è  To include this lockfile in your package distribution:")
        print("\nAdd to your pyproject.toml:")
        print("")
        print("  [tool.hatch.build]  # or similar for your build system")
        print("  include = [")
        print('      "src/yourpackage/**/*.py",  # your existing patterns')
        print('      "src/yourpackage/**/*.lock",  # add this line')
        print("  ]")
        print("")
        print("Or if using setuptools with setup.py, add to MANIFEST.in:")
        print("  include src/yourpackage/*.lock")

    print("\nüí° Use 'llmring lock chat' for conversational lockfile management")

    return 0


def cmd_bind(args):
    """Bind an alias to one or more models (with fallback support)."""
    # Load or create lockfile
    # Find the package directory where lockfile should be
    package_dir = Lockfile.find_package_directory()
    if package_dir:
        lockfile_path = package_dir / LOCKFILE_NAME
    else:
        # Fall back to current directory if no package found
        lockfile_path = Path(LOCKFILE_NAME)

    if lockfile_path.exists():
        lockfile = Lockfile.load(lockfile_path)
    else:
        print(f"No lockfile found at {lockfile_path}")
        print("Creating a new lockfile...")
        lockfile = Lockfile.create_default()

    # Parse model(s) - can be comma-separated for fallbacks
    models = args.model  # Already a string, can be comma-separated

    # Set binding
    lockfile.set_binding(args.alias, models, profile=args.profile)

    # Save
    lockfile.save(lockfile_path)

    profile_name = args.profile or lockfile.default_profile

    # Display what was bound
    if "," in models:
        model_list = [m.strip() for m in models.split(",")]
        print(f"‚úÖ Bound '{args.alias}' ‚Üí {model_list[0]} in profile '{profile_name}'")
        print(f"   Fallbacks: {', '.join(model_list[1:])}")
    else:
        print(f"‚úÖ Bound '{args.alias}' ‚Üí '{models}' in profile '{profile_name}'")

    return 0


def cmd_aliases(args):
    """List aliases from lockfile."""
    from .cli_utils import load_lockfile_or_exit, print_aliases

    # Load lockfile with consistent error handling
    lockfile_path, lockfile = load_lockfile_or_exit(require_exists=True)
    if not lockfile:
        return 1

    # Use the utility function to print aliases
    print_aliases(lockfile, args.profile)
    return 0


async def cmd_lock_validate(args):
    """Validate lockfile against registry."""
    # Find the package directory where lockfile should be
    package_dir = Lockfile.find_package_directory()
    if package_dir:
        lockfile_path = package_dir / LOCKFILE_NAME
    else:
        # Fall back to current directory if no package found
        lockfile_path = Path(LOCKFILE_NAME)

    if not lockfile_path.exists():
        print(f"Error: No llmring.lock found at {lockfile_path}")
        print("Run 'llmring lock init' to create one.")
        return 1

    lockfile = Lockfile.load(lockfile_path)
    registry = RegistryClient()

    print("Validating lockfile bindings...")

    valid = True
    for profile_name, profile in lockfile.profiles.items():
        if profile.bindings:
            print(f"\nProfile '{profile_name}':")
            for binding in profile.bindings:
                # Validate model exists in registry
                try:
                    is_valid = await registry.validate_model(binding.provider, binding.model)
                    status = "‚úÖ" if is_valid else "‚ùå"
                    print(f"  {status} {binding.alias} ‚Üí {binding.model_ref}")
                    if not is_valid:
                        valid = False
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  {binding.alias} ‚Üí {binding.model_ref} (couldn't validate: {e})")

    if valid:
        print("\n‚úÖ All bindings are valid")
        return 0
    else:
        print("\n‚ùå Some bindings are invalid")
        return 1


async def cmd_lock_bump_registry(args):
    """Update pinned registry versions to latest."""
    # Find the package directory where lockfile should be
    package_dir = Lockfile.find_package_directory()
    if package_dir:
        lockfile_path = package_dir / LOCKFILE_NAME
    else:
        # Fall back to current directory if no package found
        lockfile_path = Path(LOCKFILE_NAME)

    if not lockfile_path.exists():
        print(f"Error: No llmring.lock found at {lockfile_path}")
        print("Run 'llmring lock init' to create one.")
        return 1

    lockfile = Lockfile.load(lockfile_path)
    registry = RegistryClient()

    print("Updating registry versions...")

    for profile_name, profile in lockfile.profiles.items():
        # Get unique providers from bindings
        providers = set(b.provider for b in profile.bindings)

        for provider in providers:
            try:
                current_version = await registry.get_current_version(provider)
                old_version = profile.registry_versions.get(provider, 0)

                if current_version > old_version:
                    profile.registry_versions[provider] = current_version
                    print(f"  {provider}: v{old_version} ‚Üí v{current_version}")
                else:
                    print(f"  {provider}: v{current_version} (unchanged)")

            except Exception as e:
                print(f"  {provider}: Failed to get version ({e})")

    # Save updated lockfile
    lockfile.save(lockfile_path)
    print(f"\n‚úÖ Updated {lockfile_path}")

    return 0


def format_models_for_prompt(models_data: dict, providers_data: dict) -> str:
    """
    Format models from registry into a concise summary for system prompt.

    Note: Expects fields from LockfileManagerTools.list_models() output format
    including aliases: input_cost, output_cost, supports_functions, supports_vision.
    These are added by list_models() to simplify access to nested pricing fields.

    Args:
        models_data: Dict from LockfileManagerTools.list_models()
        providers_data: Dict from LockfileManagerTools.get_available_providers()

    Returns:
        Formatted string with model information
    """
    configured_providers = set(providers_data.get("configured", []))
    models = models_data.get("models", [])

    if not models:
        return "(No models available from registry)"

    # Group models by provider
    by_provider = {}
    for m in models:
        provider = m.get("provider", "unknown")
        if provider not in by_provider:
            by_provider[provider] = []
        by_provider[provider].append(m)

    lines = []
    for provider in sorted(by_provider.keys()):
        has_key = provider in configured_providers
        status = "‚úì CONFIGURED" if has_key else "‚úó no API key"
        lines.append(f"\n{provider.upper()} ({status}):")

        # Show top 5 models per provider (most relevant)
        provider_models = by_provider[provider][:5]
        for m in provider_models:
            # Format: model_name | context | cost/1M in/out | capabilities
            ctx = (
                f"{m['context_window'] // 1000}K"
                if m.get("context_window") and m["context_window"] > 0
                else "?"
            )
            cost_in = f"${m['input_cost']:.2f}" if m.get("input_cost") else "?"
            cost_out = f"${m['output_cost']:.2f}" if m.get("output_cost") else "?"
            caps = []
            if m.get("supports_vision"):
                caps.append("vision")
            if m.get("supports_functions"):
                caps.append("tools")
            caps_str = f" [{','.join(caps)}]" if caps else ""

            lines.append(
                f"  ‚Ä¢ {m['model_name']:<25} {ctx:>5} ctx | "
                f"{cost_in:>6}/{cost_out:>6} per 1M{caps_str}"
            )

        if len(by_provider[provider]) > 5:
            remaining = len(by_provider[provider]) - 5
            lines.append(f"  ... and {remaining} more models")

    return "\n".join(lines)


async def cmd_lock_chat(args):
    """Conversational lockfile management using MCP chat interface."""
    from pathlib import Path

    # Import MCP chat app
    from llmring.mcp.client.chat.app import MCPChatApp

    print("ü§ñ LLMRing Conversational Lockfile Manager")
    print("=" * 50)

    # Find the user's package lockfile path (inside their package for distribution)
    package_dir = Lockfile.find_package_directory()
    if package_dir:
        user_lockfile_path = package_dir / LOCKFILE_NAME
        print(f"üì¶ Package directory: {package_dir}")
    else:
        # Fall back to current directory if no package found
        user_lockfile_path = Path.cwd() / LOCKFILE_NAME
        print("üìÅ No package found, using current directory")
    print(f"üìÑ Managing lockfile: {user_lockfile_path}")

    # If no server URL provided, we'll use embedded server
    if not args.server_url:
        # The stdio transport will be handled by the chat app directly
        # Pass the lockfile path as an argument to the server
        # This will override any LLMRING_LOCKFILE_PATH environment variable
        server_url = (
            f"stdio://python -m llmring.mcp.server.lockfile_server --lockfile {user_lockfile_path}"
        )
        server_process = None  # stdio client will manage the process
        print("Will use embedded lockfile MCP server via stdio")
    else:
        server_url = args.server_url
        server_process = None

    # Set the bundled lockfile path AFTER configuring the server URL
    # This ensures the 'advisor' alias works but doesn't affect the server
    os.environ["LLMRING_LOCKFILE_PATH"] = str(Lockfile.get_package_lockfile_path())

    # Fetch registry models for system prompt prepopulation
    from llmring.mcp.tools.lockfile_manager import LockfileManagerTools

    print("üìä Loading available models from registry...")
    try:
        # Create tools instance to fetch models
        tools_instance = LockfileManagerTools(lockfile_path=user_lockfile_path)

        # Fetch models and providers in parallel for faster startup
        models_data, providers_data = await asyncio.gather(
            tools_instance.list_models(), tools_instance.get_available_providers()
        )

        # Format for system prompt
        models_summary = format_models_for_prompt(models_data, providers_data)
        configured_count = len(providers_data.get("configured", []))
        total_count = models_data.get("total_count", 0)
        print(f"‚úÖ Loaded {total_count} models from {configured_count} configured provider(s)\n")
    except Exception as e:
        import logging

        logging.warning(f"Could not fetch models for system prompt: {e}")
        print("‚ö†Ô∏è  Could not fetch models from registry")
        models_summary = "(Registry unavailable - use list_models tool to fetch current models)"

    # System prompt that explains multi-model aliases and profiles clearly
    system_prompt = f"""You are the LLMRing Lockfile Manager assistant. You help users manage their LLM aliases and model configurations.

AVAILABLE MODELS FROM REGISTRY (snapshot at startup):
{models_summary}

The models listed above are currently available from the registry. Use this information to make informed recommendations.
If a provider shows "‚úó no API key", models from that provider won't work until the user configures their API key.

NOTE: This is a snapshot taken at startup. You can call the list_models tool to fetch fresh or complete model information if needed.

IMPORTANT: Understanding Model Pools in LLMRing
================================================
LLMRing aliases can be bound to a POOL OF MODELS - multiple prioritized alternatives that ensure your code always works regardless of which API keys you have configured.

IMPORTANT: Understanding Profiles
=================================
LLMRing supports PROFILES for environment-specific configurations (dev, staging, prod, test, etc.).
The same alias can have different models in different profiles:
- 'default' profile: Standard configuration
- 'dev' profile: Cheaper/faster models for development
- 'prod' profile: High-quality models for production
- 'test' profile: Minimal models for automated testing

Users can specify profiles when adding aliases: add_alias(alias="fast", models="...", profile="dev")

How Model Pools Work:
1. You provide multiple models in priority order: "anthropic:claude-3-haiku,openai:gpt-4o-mini"
2. LLMRing intelligently selects the first available model based on configured API keys
3. This ensures seamless operation across different environments and teams

Why Use Model Pools?
- **Flexibility**: Same code works whether you have OpenAI, Anthropic, or both API keys
- **Resilience**: Automatic provider switching if one service is unavailable
- **Collaboration**: Team members can use different providers without code changes
- **Cost Optimization**: Prioritize cheaper models while maintaining alternatives

Example Model Pool Configurations:
- "fast": "anthropic:claude-3-haiku,openai:gpt-4o-mini" ‚Üí Budget-friendly, fast responses
- "advisor": "anthropic:claude-3-5-sonnet,openai:gpt-4o" ‚Üí High-quality reasoning
- "coder": "anthropic:claude-3-5-sonnet,openai:gpt-4o,google:gemini-pro" ‚Üí Maximum availability

Example Profile Usage:
- add_alias(alias="fast", models="openai:gpt-3.5-turbo", profile="test") ‚Üí Test profile
- add_alias(alias="fast", models="openai:gpt-4o-mini", profile="dev") ‚Üí Dev profile
- add_alias(alias="fast", models="anthropic:claude-3-5-sonnet", profile="prod") ‚Üí Prod profile
- list_aliases(profile="dev") ‚Üí Show dev profile aliases

Smart Selection Examples:
- Only OpenAI key available: Uses openai models from the pool
- Both keys available: Uses first model in priority order
- Only Anthropic key available: Uses anthropic models from the pool

When helping users:
- Remember that the 'models' parameter (accepts single or multiple models)
- DO suggest model pools for better flexibility and resilience
- DO help users understand priority ordering in their model pools
- DON'T suggest single models when users want flexibility
- DO NOT

You have access to tools that help manage the lockfile. Use them to provide accurate information and help users configure their model pools effectively.

Your task is to guide users in creating or updating their lockfile, where they define their profiles and their alias pools.

VERY IMPORTANT: you need to understand the use they want to give to the models and suggest the best option, but DO NOT add or delete anything until you are told to do so.
    """

    try:
        # Create and run MCP chat app with system prompt
        app = MCPChatApp(
            mcp_server_url=server_url, llm_model=args.model, system_prompt=system_prompt
        )

        # Minimal initialization
        await app.initialize_async()

        # Run the chat interface
        await app.run()

    finally:
        # Clean up server process if we started it
        if server_process:
            server_process.terminate()
            server_process.wait()
            print("\n‚úÖ Stopped lockfile MCP server")

    return 0


async def cmd_list_models(args):
    """List available models."""
    async with LLMRing() as ring:
        models = await ring.get_available_models()

        if args.provider:
            # Filter by provider
            models = {k: v for k, v in models.items() if k == args.provider}

        print(format_model_table(models, show_all=True))


async def cmd_chat(args):
    """Send a chat message to an LLM."""
    model_input = args.model
    profile_name = os.environ.get("LLMRING_PROFILE", args.profile)

    async with LLMRing() as ring:
        # Preview alias resolution without losing alias metadata
        if ":" not in model_input:
            try:
                resolved_preview = ring.resolve_alias(model_input, profile_name)
                print(f"[Using alias '{model_input}' ‚Üí '{resolved_preview}']")
            except ValueError as exc:
                print(f"Error: {exc}")
                return 1

        # Create message
        messages = [Message(role="user", content=args.message)]
        if args.system:
            messages.insert(0, Message(role="system", content=args.system))

        # Create request
        request = LLMRequest(
            messages=messages,
            model=model_input,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
        )

        try:
            # Send request
            # Handle streaming response
            if args.stream:
                # Stream response chunks
                import sys

                full_content = ""
                accumulated_usage = None
                final_model = None
                finish_reason = None

                stream = ring.chat_stream(request, profile=profile_name)

                async for chunk in stream:
                    if chunk.delta:
                        if not args.json:
                            # Print chunks as they arrive
                            sys.stdout.write(chunk.delta)
                            sys.stdout.flush()
                        full_content += chunk.delta

                    # Capture final usage stats
                    if chunk.usage:
                        accumulated_usage = chunk.usage
                    if chunk.model:
                        final_model = chunk.model
                    if chunk.finish_reason:
                        finish_reason = chunk.finish_reason

                if args.json:
                    # For JSON output, collect all chunks first
                    output_model = final_model or model_input
                    print(
                        json.dumps(
                            {
                                "content": full_content,
                                "model": output_model,
                                "usage": accumulated_usage,
                                "finish_reason": finish_reason,
                            },
                            indent=2,
                        )
                    )
                else:
                    # Print newline after streaming
                    print()

                    if args.verbose and accumulated_usage:
                        output_model = final_model or model_input
                        print(f"\n[Model: {output_model}]")
                        print(
                            f"[Tokens: {accumulated_usage.get('prompt_tokens', 0)} in, {accumulated_usage.get('completion_tokens', 0)} out]"
                        )
                        if "cost" in accumulated_usage:
                            print(f"[Cost: ${accumulated_usage['cost']:.6f}]")
            else:
                # Non-streaming response (existing code)
                # Display response
                response = await ring.chat(request, profile=profile_name)
                if args.json:
                    print(
                        json.dumps(
                            {
                                "content": response.content,
                                "model": response.model,
                                "usage": response.usage,
                                "finish_reason": response.finish_reason,
                            },
                            indent=2,
                        )
                    )
                else:
                    print(response.content)

                    if args.verbose and response.usage:
                        print(f"\n[Model: {response.model}]")
                        print(
                            f"[Tokens: {response.usage.get('prompt_tokens', 0)} in, {response.usage.get('completion_tokens', 0)} out]"
                        )
                        if "cost" in response.usage:
                            print(f"[Cost: ${response.usage['cost']:.6f}]")

        except Exception as e:
            print(f"Error: {e}")
            return 1

    return 0


async def cmd_info(args):
    """Show information about a specific model."""
    async with LLMRing() as ring:
        try:
            # Get enhanced info including registry data
            info = await ring.get_enhanced_model_info(args.model)

            if args.json:
                print(json.dumps(info, indent=2, default=str))
            else:
                print(f"Model: {info['model']}")
                print(f"Provider: {info['provider']}")
                print(f"Supported: {info['supported']}")

                # Show additional info if available
                if "display_name" in info:
                    print(f"Display Name: {info['display_name']}")
                if "description" in info:
                    print(f"Description: {info['description']}")
                if "max_input_tokens" in info:
                    print(f"Max Input: {info['max_input_tokens']:,} tokens")
                if "max_output_tokens" in info:
                    print(f"Max Output: {info['max_output_tokens']:,} tokens")
                if "dollars_per_million_tokens_input" in info:
                    print(f"Input Cost: ${info['dollars_per_million_tokens_input']:.2f}/M tokens")
                if "dollars_per_million_tokens_output" in info:
                    print(f"Output Cost: ${info['dollars_per_million_tokens_output']:.2f}/M tokens")
                if "supports_vision" in info and info["supports_vision"]:
                    print("Supports: Vision")
                if "supports_function_calling" in info and info["supports_function_calling"]:
                    print("Supports: Function Calling")
                if "supports_json_mode" in info and info["supports_json_mode"]:
                    print("Supports: JSON Mode")
                if "is_default" in info:
                    print(f"Default: {info['is_default']}")

        except Exception as e:
            print(f"Error: {e}")
            return 1

        return 0


# Push/pull commands removed per source-of-truth v3.8
# Aliases are managed entirely locally in each codebase's lockfile


async def cmd_stats(args):
    """Show usage statistics from server."""
    print("‚ùå The 'stats' command has been removed.")
    print("\nThis command was dependent on the receipts feature, which has been removed.")
    print("Usage statistics and cost tracking will be reimplemented in a future version.")
    return 1


async def cmd_export(args):
    """Export data from server to file."""
    print("‚ùå The 'export' command has been removed.")
    print("\nThis command was dependent on the receipts feature, which has been removed.")
    print("Data export functionality will be reimplemented in a future version.")
    return 1


async def cmd_cache_clear(args):
    """Clear the registry cache."""
    from llmring.registry import RegistryClient

    registry = RegistryClient()
    registry.clear_cache()
    print("‚úÖ Registry cache cleared successfully")
    print("Next model lookups will fetch fresh data from the registry")
    return 0


async def cmd_cache_info(args):
    """Show cache information."""
    from datetime import datetime, timezone

    from llmring.registry import RegistryClient

    registry = RegistryClient()
    cache_dir = registry.cache_dir

    print(f"üìÅ Cache directory: {cache_dir}")
    print(f"‚è±Ô∏è  Cache TTL: {registry.CACHE_DURATION_HOURS} hours")

    # List cache files and their ages
    cache_files = list(cache_dir.glob("*.json"))
    if cache_files:
        print(f"\nüìÑ Cached files ({len(cache_files)} total):")
        for cache_file in sorted(cache_files):
            # Get file age
            mtime = datetime.fromtimestamp(cache_file.stat().st_mtime, tz=timezone.utc)
            age_hours = (datetime.now(timezone.utc) - mtime).total_seconds() / 3600

            # Check if still valid
            is_valid = age_hours < registry.CACHE_DURATION_HOURS
            status = "‚úÖ valid" if is_valid else "‚ùå stale"

            print(f"  ‚Ä¢ {cache_file.name}: {age_hours:.1f}h old ({status})")
    else:
        print("\nüìÑ No cached files")

    # Show cache size
    total_size = sum(f.stat().st_size for f in cache_files) if cache_files else 0
    print(f"\nüíæ Total cache size: {total_size / 1024:.1f} KB")

    return 0


async def cmd_server_init(args):
    """Bootstrap local server configuration and write an env file."""
    env_file = Path(args.env_file).expanduser()

    server_url = (args.server or os.getenv("LLMRING_SERVER_URL") or "http://localhost:8000").rstrip(
        "/"
    )
    api_key = (args.api_key or os.getenv("LLMRING_API_KEY") or _generate_api_key()).strip()

    print(format_info(f"Checking server health at {server_url}..."))
    try:
        async with ServerClient(base_url=server_url, api_key=api_key) as client:
            if not args.skip_health:
                health = await client.get("/health")
                status = health.get("status", "unknown")
                version = health.get("version") or health.get("name")
                print(
                    format_success(f"Server responded: status={status}, version={version or 'n/a'}")
                )
    except httpx.RequestError as exc:
        print(format_error(f"Could not reach {server_url}: {exc}"))
        if not args.skip_health:
            print("Use --skip-health-check to skip validation if the server is not running yet.")
        return 1
    except httpx.HTTPStatusError as exc:
        detail = None
        try:
            detail = exc.response.json().get("detail")
        except Exception:
            detail = exc.response.text
        print(format_error(f"Server responded with {exc.response.status_code}: {detail}"))
        return 1

    try:
        _write_env_file(env_file, server_url, api_key, overwrite=args.force)
    except FileExistsError as exc:
        print(format_error(str(exc)))
        print("Use --force to overwrite or specify a different path with --env-file.")
        return 1

    os.environ["LLMRING_SERVER_URL"] = server_url
    os.environ["LLMRING_API_KEY"] = api_key

    print()
    print(format_success(f"‚úÖ Saved configuration to {env_file}"))
    print(f"   LLMRING_SERVER_URL={server_url}")
    print(f"   LLMRING_API_KEY={api_key}")
    print()
    print("Next steps:")
    print(f"  source {env_file}  # load variables into your shell")
    suggested_alias = None
    if Path(LOCKFILE_NAME).exists():
        try:
            from llmring.lockfile_core import Lockfile

            lockfile = Lockfile.load(Path(LOCKFILE_NAME))
            profile = lockfile.profiles.get(lockfile.default_profile)
            if profile and profile.bindings:
                binding = profile.bindings[0]
                suggested_alias = binding.alias
        except Exception:
            pass

    if suggested_alias:
        print(
            f'  llmring chat "Summarize the latest usage logs and suggest optimizations" '
            f"--model {suggested_alias}"
        )
    else:
        print("  llmring lock init  # create a lockfile with aliases such as 'fast'")
        print(
            '  llmring chat "Summarize the latest usage logs and suggest optimizations" '
            "--model fast"
        )
    return 0


async def cmd_server_status(args):
    """Display current server configuration and health."""
    server_url, api_key, used_saas = _resolve_server_settings(
        server_override=args.server, api_key_override=args.api_key, enable_saas_fallback=True
    )

    env_file = Path(args.env_file).expanduser()
    file_config = _load_env_file(env_file)

    print("Server configuration")
    print("--------------------")
    print(f"  server url : {server_url or '(not configured)'}")
    if used_saas:
        print("  source     : SaaS fallback (LLMRING_PREFER_SAAS)")
    elif args.server:
        print("  source     : CLI override (--server)")
    elif os.getenv("LLMRING_SERVER_URL"):
        print("  source     : environment variable")
    elif env_file.exists():
        print(f"  source     : {env_file}")

    print(f"  api key    : {_mask_api_key(api_key)}")
    if api_key is None and file_config.get("LLMRING_API_KEY"):
        print("               (present in env file but not exported in session)")

    if not server_url:
        print()
        print("Run `llmring server init` to create a local configuration.")
        return 1

    print()
    print(format_info(f"Pinging {server_url}/health ..."))
    try:
        async with httpx.AsyncClient(base_url=server_url, timeout=5.0) as client:
            response = await client.get("/health")
            response.raise_for_status()
            payload = response.json()
            print(
                format_success(
                    f"status={payload.get('status', 'unknown')} database={payload.get('database', 'n/a')}"
                )
            )
    except httpx.RequestError as exc:
        print(format_error(f"Connection failed: {exc}"))
        return 1
    except httpx.HTTPStatusError as exc:
        print(
            format_error(
                f"Health endpoint returned {exc.response.status_code}: {exc.response.text}"
            )
        )
        return 1

    if api_key:
        try:
            async with ServerClient(base_url=server_url, api_key=api_key) as client:
                await client.get("/api/v1/stats")
            print(format_success("API key accepted by usage endpoint."))
        except httpx.HTTPStatusError as exc:
            if exc.response.status_code == 401:
                print(
                    format_warning(
                        "API key was rejected (401). Generate a new one with `llmring server key rotate`."
                    )
                )
            else:
                print(
                    format_warning(
                        f"Usage endpoint returned {exc.response.status_code}: {exc.response.text}"
                    )
                )
        except httpx.RequestError as exc:
            print(format_warning(f"Could not validate API key: {exc}"))
    else:
        print("No API key configured; logging and MCP commands will be disabled.")

    return 0


def cmd_server_key_create(args):
    """Generate a new API key (optional env-file update)."""
    env_path = Path(args.env_file).expanduser()
    server_url, _, _ = _resolve_server_settings(enable_saas_fallback=False)
    if not server_url:
        # Fall back to env file or arg
        env_data = _load_env_file(env_path)
        server_url = (
            args.server or env_data.get("LLMRING_SERVER_URL") or "http://localhost:8000"
        ).rstrip("/")

    new_key = _generate_api_key()
    print(format_success("Generated new API key"))
    print(f"  {new_key}")

    if args.update_env:
        try:
            _write_env_file(env_path, server_url, new_key, overwrite=True)
        except FileExistsError:
            # Overwrite=True ensures file is rewritten regardless
            pass
        os.environ["LLMRING_API_KEY"] = new_key
        os.environ["LLMRING_SERVER_URL"] = server_url
    print(format_info(f"Updated {env_path} with new credentials."))

    return 0


async def cmd_server_stats(args):
    """Fetch usage statistics from the server."""
    env = await _ensure_server_env(server_override=args.server, api_key_override=args.api_key)
    if not env:
        return 1
    server_url, api_key = env

    from llmring.mcp.http_client import MCPHttpClient  # reuse BaseHTTPClient

    client = MCPHttpClient(base_url=server_url, api_key=api_key, timeout=30.0)
    try:
        stats = await client.get(
            "/api/v1/stats",
            params={
                "start_date": args.start_date,
                "end_date": args.end_date,
                "group_by": args.group_by,
            },
        )
    except Exception as exc:
        print(format_error(f"Failed to fetch usage stats: {exc}"))
        return 1
    finally:
        await client.close()

    if args.json:
        print(json.dumps(stats, indent=2))
        return 0

    if not stats:
        print("No usage data available.")
        return 0

    print("Usage statistics")
    print("----------------")
    for row in stats:
        date = row.get("date") or row.get("period")
        total_cost = row.get("total_cost")
        total_tokens = row.get("total_tokens")
        total_calls = row.get("total_calls")
        print(f"{date}: {total_calls} calls, {total_tokens} tokens, cost ${total_cost:.4f}")

    return 0


async def cmd_server_logs(args):
    """Fetch raw usage logs."""
    env = await _ensure_server_env(server_override=args.server, api_key_override=args.api_key)
    if not env:
        return 1
    server_url, api_key = env

    from llmring.mcp.http_client import MCPHttpClient

    client = MCPHttpClient(base_url=server_url, api_key=api_key, timeout=30.0)
    try:
        logs = await client.get(
            "/api/v1/logs",
            params={
                "limit": args.limit,
                "offset": args.offset,
                "alias": args.alias,
                "model": args.model,
                "origin": args.origin,
                "start_date": args.start_date,
                "end_date": args.end_date,
            },
        )
    except Exception as exc:
        print(format_error(f"Failed to fetch logs: {exc}"))
        return 1
    finally:
        await client.close()

    if args.json or args.output == "json":
        if args.output_file:
            Path(args.output_file).write_text(json.dumps(logs, indent=2), encoding="utf-8")
            print(format_success(f"Wrote {len(logs)} records to {args.output_file}"))
        else:
            print(json.dumps(logs, indent=2))
        return 0

    if args.output == "csv":
        import csv

        if args.output_file:
            handle = open(args.output_file, "w", encoding="utf-8", newline="")
        else:
            handle = sys.stdout

        fieldnames = [
            "logged_at",
            "provider",
            "model",
            "alias",
            "profile",
            "origin",
            "input_tokens",
            "output_tokens",
            "cost",
        ]
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for row in logs or []:
            writer.writerow({k: row.get(k, "") for k in fieldnames})

        if args.output_file:
            handle.close()
            print(format_success(f"Wrote {len(logs or [])} records to {args.output_file}"))
        return 0

    # Default human-readable output
    if not logs:
        print("No log entries found.")
        return 0

    print("Recent usage logs")
    print("-----------------")
    for row in logs:
        print(
            f"{row.get('logged_at')} :: {row.get('origin','n/a')} :: "
            f"{row.get('alias') or row.get('model')} "
            f"{row.get('input_tokens')}‚Üí{row.get('output_tokens')} "
            f"${row.get('cost', 0.0):.4f}"
        )

    return 0


async def cmd_server_conversations(args):
    """List conversations or show a specific conversation."""
    env = await _ensure_server_env(server_override=args.server, api_key_override=args.api_key)
    if not env:
        return 1
    server_url, api_key = env

    from llmring.mcp.http_client import MCPHttpClient

    client = MCPHttpClient(base_url=server_url, api_key=api_key, timeout=30.0)
    try:
        if args.conversation_id:
            convo = await client.get(f"/api/v1/conversations/{args.conversation_id}")
            if args.messages:
                messages = await client.get(
                    f"/api/v1/conversations/{args.conversation_id}/messages",
                    params={"limit": args.limit, "offset": args.offset},
                )
                convo["messages"] = messages
            data = convo
        else:
            data = await client.get(
                "/api/v1/conversations/",
                params={"limit": args.limit, "offset": args.offset},
            )
    except Exception as exc:
        print(format_error(f"Failed to retrieve conversations: {exc}"))
        return 1
    finally:
        await client.close()

    if args.json:
        print(json.dumps(data, indent=2))
        return 0

    if isinstance(data, dict) and data.get("id"):
        convo = data
        print(f"Conversation {convo['id']}")
        print(f" title      : {convo.get('title')}")
        print(f" model      : {convo.get('model_alias')}")
        print(f" created_at : {convo.get('created_at')}")
        print(f" updated_at : {convo.get('updated_at')}")
        print(f" origin     : {convo.get('origin')}")
        msgs = convo.get("messages", [])
        if msgs:
            print("\nMessages:")
            for msg in msgs:
                preview = msg.get("content")
                if preview and len(preview) > 80:
                    preview = preview[:77] + "..."
                print(
                    f"  [{msg.get('role')}] {preview or '(no content)'} "
                    f"(in={msg.get('input_tokens')} out={msg.get('output_tokens')})"
                )
    else:
        items = data or []
        if not items:
            print("No conversations found.")
            return 0
        print("Conversations")
        print("-------------")
        for row in items:
            print(
                f"{row.get('id')} :: {row.get('title') or '(untitled)'} "
                f"model={row.get('model_alias')} updated={row.get('updated_at')}"
            )

    return 0


def cmd_server_key_list(args):
    """Show currently configured API keys from environment and env file."""
    env_path = Path(args.env_file).expanduser()
    server_url, api_key, used_saas = _resolve_server_settings(enable_saas_fallback=True)
    data = _load_env_file(env_path)

    print("Active session")
    print("--------------")
    print(f"  server url : {server_url or '(not configured)'}")
    if used_saas:
        print("  source     : SaaS fallback (LLMRING_PREFER_SAAS)")
    print(f"  api key    : {_mask_api_key(api_key)}")
    print()
    print(f"Env file: {env_path}")
    if not env_path.exists():
        print("  (file not found)")
    else:
        print(f"  server url : {data.get('LLMRING_SERVER_URL', '(missing)')}")
        print(f"  api key    : {_mask_api_key(data.get('LLMRING_API_KEY'))}")
    return 0


def cmd_server_key_rotate(args):
    """Generate a new API key and overwrite the env file."""
    env_path = Path(args.env_file).expanduser()
    env_data = _load_env_file(env_path)
    previous_key = env_data.get("LLMRING_API_KEY") or os.getenv("LLMRING_API_KEY")
    server_url = (
        args.server
        or env_data.get("LLMRING_SERVER_URL")
        or os.getenv("LLMRING_SERVER_URL")
        or "http://localhost:8000"
    ).rstrip("/")

    new_key = _generate_api_key()
    _write_env_file(env_path, server_url, new_key, overwrite=True)
    os.environ["LLMRING_SERVER_URL"] = server_url
    os.environ["LLMRING_API_KEY"] = new_key

    print(format_success("Rotated API key"))
    print(f"  new key: {new_key}")
    if previous_key:
        print(f"  old key: {_mask_api_key(previous_key)} (please revoke anywhere else it is used)")
    print(format_info(f"Updated {env_path}. Source it again to load the new key."))
    return 0


async def cmd_register(args):
    """Register with LLMRing SaaS and provision an API key via CLI."""
    import httpx

    from llmring.server_client import ServerClient

    server_url = args.server or os.getenv("LLMRING_SERVER_URL") or "https://api.llmring.ai"
    server_url = server_url.rstrip("/")

    non_interactive = bool(getattr(args, "non_interactive", False))

    email = args.email
    if not email and not non_interactive:
        email = input("Email address: ").strip()

    if not email:
        print(format_error("Email is required to create an account."))
        return 1

    organization = args.org
    if organization is None and not non_interactive:
        org_input = input("Organization (optional): ").strip()
        organization = org_input or None

    project_name = args.project_name
    if project_name is None and not non_interactive:
        proj_input = input("Project name [Default Project]: ").strip()
        project_name = proj_input or None

    accept_terms = bool(args.accept_terms)
    if not accept_terms and not non_interactive:
        confirmation = input("Do you accept the LLMRing Terms of Service? [y/N]: ").strip()
        accept_terms = confirmation.lower() in ("y", "yes")

    if not accept_terms:
        print(format_error("You must accept the Terms of Service to create an account."))
        return 1

    payload = {
        "email": email,
        "organization": organization,
        "project_name": project_name,
        "accept_terms": True,
    }

    print(format_info(f"Registering with LLMRing at {server_url}..."))

    try:
        async with ServerClient(base_url=server_url) as client:
            result = await client.post("/api/v1/cli/register", json=payload)
    except httpx.HTTPStatusError as exc:
        try:
            detail = exc.response.json().get("detail")
        except Exception:
            detail = exc.response.text
        if exc.response.status_code == 409:
            print(format_warning(f"{detail} Visit {server_url}/login to manage your account."))
        else:
            print(format_error(f"Registration failed: {detail}"))
        return 1
    except httpx.RequestError as exc:
        print(format_error(f"Could not reach {server_url}: {exc}"))
        return 1

    api_key = result["api_key"]
    dashboard_url = result.get("dashboard_url", f"{server_url}/dashboard")
    api_key_active = result.get("api_key_active", True)

    print()
    print(format_success("Account created!"))
    print(f"üîë  API key: {api_key}")
    if not api_key_active:
        print(format_warning("API key is pending email verification and cannot be used yet."))
    print("üì¨  Verification email sent. Please verify to unlock dashboard access.")
    print(
        format_info(
            f"Dashboard: {dashboard_url}\n"
            "Add the API key to your environment:\n"
            f'    export LLMRING_API_KEY="{api_key}"\n'
            "Keep this key secret‚Äîit cannot be shown again."
        )
    )

    return 0


# ============================================================================
# MCP CLI Commands (Managed persistence via server)
# ============================================================================


async def _ensure_server_env(
    *,
    server_override: Optional[str] = None,
    api_key_override: Optional[str] = None,
    require_api_key: bool = True,
) -> tuple[str, Optional[str]] | None:
    """Validate server URL/API key configuration for commands that need backend access."""
    server_url, api_key, used_saas = _resolve_server_settings(
        server_override, api_key_override, enable_saas_fallback=True
    )

    if not server_url:
        print("‚ùå Error: No llmring-server configured.")
        print("Set LLMRING_SERVER_URL or run `llmring server init` to create a local config.")
        return None

    if require_api_key and not api_key:
        print("‚ùå Error: LLMRING_API_KEY environment variable required for this command.")
        if used_saas:
            print("Set it to your SaaS project API key.")
        else:
            print("Generate one with `llmring server key create` or assign your own token.")
        return None

    return server_url, api_key


async def cmd_mcp_servers_list(args):
    """List MCP servers for the current project (API key scope)."""
    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        servers = await client.list_servers()
        if not servers:
            print("(No MCP servers registered)")
            return 0

        show_inactive = bool(getattr(args, "all", False))
        for s in servers:
            if (s.get("is_active") is False) and not show_inactive:
                continue
            print(
                f"- {s.get('name')}  id={s.get('id')}  type={s.get('transport_type')}  url={s.get('url')}"
            )
        return 0
    except Exception as e:
        print(f"‚ùå Error listing MCP servers: {e}")
        return 1


async def cmd_mcp_servers_register(args):
    """Register a new MCP server for this project."""
    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        created = await client.register_server(
            name=args.name, url=args.url, transport_type=args.transport
        )
        print("‚úÖ Registered MCP server")
        print(json.dumps(created, indent=2, default=str))
        return 0
    except Exception as e:
        print(f"‚ùå Error registering MCP server: {e}")
        return 1


async def cmd_mcp_tools_list(args):
    """List MCP tools; optionally filter by server id."""
    from uuid import UUID

    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        server_id = None
        if getattr(args, "server_id", None):
            try:
                server_id = UUID(args.server_id)
            except Exception:
                print("‚ùå Invalid --server-id (must be UUID)")
                return 1

        tools = await client.list_tools(server_id=server_id)
        if not tools:
            print("(No MCP tools found)")
            return 0

        for t in tools:
            name = t.get("name")
            tid = t.get("id")
            server = t.get("server") or {}
            sname = server.get("name") or "?"
            print(f"- {name}  id={tid}  server={sname}")
        return 0
    except Exception as e:
        print(f"‚ùå Error listing MCP tools: {e}")
        return 1


async def cmd_mcp_prompts_list(args):
    """List MCP prompts; optionally filter by server id."""
    from uuid import UUID

    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        server_id = None
        if getattr(args, "server_id", None):
            try:
                server_id = UUID(args.server_id)
            except Exception:
                print("‚ùå Invalid --server-id (must be UUID)")
                return 1

        prompts = await client.list_prompts(server_id=server_id)
        if not prompts:
            print("(No MCP prompts found)")
            return 0

        for p in prompts:
            print(f"- {p.get('name')}  id={p.get('id')}  server={p.get('server_id')}")
        return 0
    except Exception as e:
        print(f"‚ùå Error listing MCP prompts: {e}")
        return 1


async def cmd_mcp_prompts_register(args):
    """Append a prompt to a server by fetching current capabilities and refreshing."""
    from uuid import UUID

    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    try:
        server_id = UUID(args.server_id)
    except Exception:
        print("‚ùå Invalid --server-id (must be UUID)")
        return 1

    # Parse arguments JSON (inline or file path)
    arguments = None
    if getattr(args, "arguments", None):
        raw = args.arguments
        try:
            # If it's a path to a file, read it
            if raw.endswith(".json") and os.path.exists(raw):
                with open(raw, "r") as f:
                    import json as _json

                    arguments = _json.load(f)
            else:
                import json as _json

                arguments = _json.loads(raw)
        except Exception as e:
            print(f"‚ùå Failed to parse --arguments as JSON or file: {e}")
            return 1

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        # Fetch current capabilities
        tools = await client.list_tools(server_id=server_id)
        resources = await client.list_resources(server_id=server_id)
        prompts = await client.list_prompts(server_id=server_id)

        # Convert to refresh shapes
        tools_caps = [
            {
                "name": t.get("name"),
                "description": t.get("description"),
                "inputSchema": t.get("input_schema") or {},
            }
            for t in tools
        ]
        resources_caps = [
            {
                "uri": r.get("uri"),
                "name": r.get("name"),
                "description": r.get("description"),
                "mimeType": r.get("mime_type"),
            }
            for r in resources
        ]
        prompts_caps = [
            {
                "name": p.get("name"),
                "description": p.get("description"),
                "arguments": p.get("arguments") or {},
            }
            for p in prompts
        ]

        # Append new prompt
        prompts_caps.append(
            {
                "name": args.name,
                "description": getattr(args, "description", None),
                "arguments": arguments or {},
            }
        )

        await client.refresh_server_capabilities(
            server_id=server_id,
            tools=tools_caps,
            resources=resources_caps,
            prompts=prompts_caps,
        )

        print("‚úÖ Registered prompt")
        return 0
    except Exception as e:
        print(f"‚ùå Error registering prompt: {e}")
        return 1


async def cmd_mcp_resources_register(args):
    """Append a resource to a server by fetching current capabilities and refreshing."""
    from uuid import UUID

    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    try:
        server_id = UUID(args.server_id)
    except Exception:
        print("‚ùå Invalid --server-id (must be UUID)")
        return 1

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        # Fetch current capabilities
        tools = await client.list_tools(server_id=server_id)
        resources = await client.list_resources(server_id=server_id)
        prompts = await client.list_prompts(server_id=server_id)

        # Convert to refresh shapes
        tools_caps = [
            {
                "name": t.get("name"),
                "description": t.get("description"),
                "inputSchema": t.get("input_schema") or {},
            }
            for t in tools
        ]
        resources_caps = [
            {
                "uri": r.get("uri"),
                "name": r.get("name"),
                "description": r.get("description"),
                "mimeType": r.get("mime_type"),
            }
            for r in resources
        ]
        prompts_caps = [
            {
                "name": p.get("name"),
                "description": p.get("description"),
                "arguments": p.get("arguments") or {},
            }
            for p in prompts
        ]

        # Append new resource
        resources_caps.append(
            {
                "uri": args.uri,
                "name": getattr(args, "name", None),
                "description": getattr(args, "description", None),
                "mimeType": getattr(args, "mime_type", None),
            }
        )

        await client.refresh_server_capabilities(
            server_id=server_id,
            tools=tools_caps,
            resources=resources_caps,
            prompts=prompts_caps,
        )

        print("‚úÖ Registered resource")
        return 0
    except Exception as e:
        print(f"‚ùå Error registering resource: {e}")
        return 1


async def cmd_mcp_resources_list(args):
    """List MCP resources; optionally filter by server id."""
    from uuid import UUID

    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        server_id = None
        if getattr(args, "server_id", None):
            try:
                server_id = UUID(args.server_id)
            except Exception:
                print("‚ùå Invalid --server-id (must be UUID)")
                return 1

        resources = await client.list_resources(server_id=server_id)
        if not resources:
            print("(No MCP resources found)")
            return 0

        for r in resources:
            print(
                f"- {r.get('uri')}  id={r.get('id')}  mime={r.get('mime_type')}  server={r.get('server_id')}"
            )
        return 0
    except Exception as e:
        print(f"‚ùå Error listing MCP resources: {e}")
        return 1


async def cmd_mcp_apply(args):
    """Apply MCP configuration from a JSON manifest file."""
    from llmring.mcp.apply import ensure_from_manifest, load_manifest
    from llmring.mcp.http_client import MCPHttpClient

    env = await _ensure_server_env()
    if not env:
        return 1
    server_url, api_key = env

    try:
        manifest = load_manifest(args.file)
    except Exception as e:
        print(f"‚ùå Failed to load manifest: {e}")
        return 1

    client = MCPHttpClient(base_url=server_url, api_key=api_key)
    try:
        applied = await ensure_from_manifest(client, manifest)
        print(f"‚úÖ Applied MCP manifest for {len(applied)} server(s)")
        return 0
    except Exception as e:
        print(f"‚ùå Error applying manifest: {e}")
        return 1


def cmd_providers(args):
    """List configured providers."""
    ring = LLMRing()

    providers = []
    for provider_name in ["openai", "anthropic", "google", "ollama"]:
        try:
            provider = ring.get_provider(provider_name)
            has_key = provider is not None
        except Exception:
            has_key = False

        providers.append(
            {
                "provider": provider_name,
                "configured": has_key,
                "api_key_env": {
                    "openai": "OPENAI_API_KEY",
                    "anthropic": "ANTHROPIC_API_KEY",
                    "google": "GOOGLE_API_KEY or GEMINI_API_KEY",
                    "ollama": "(not required)",
                }.get(provider_name, ""),
            }
        )

    if args.json:
        print(json.dumps(providers, indent=2))
    else:
        print("Configured Providers:")
        print("-" * 40)
        for p in providers:
            status = "‚úì" if p["configured"] else "‚úó"
            print(f"{status} {p['provider']:<12} {p['api_key_env']}")


def format_model_table(models: dict, show_all: bool = False):
    """Format models as a readable table."""
    if not models:
        return "No models found."

    lines = []
    lines.append("Available Models:")
    lines.append("-" * 40)

    for provider, model_list in models.items():
        if model_list or show_all:
            lines.append(f"\n{provider.upper()}:")
            if model_list:
                for model in model_list:
                    lines.append(f"  - {model}")
            else:
                lines.append("  (No models available)")

    return "\n".join(lines)


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="LLMRing - Unified LLM Service CLI with Profile Support\n\nProfiles allow environment-specific configurations (dev, prod, test).\nUse --profile flag or set LLMRING_PROFILE environment variable.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    subparsers = parser.add_subparsers(dest="command", help="Commands")

    # Lock commands
    lock_parser = subparsers.add_parser("lock", help="Lockfile management")
    lock_subparsers = lock_parser.add_subparsers(dest="lock_command", help="Lock commands")

    # lock init
    init_parser = lock_subparsers.add_parser("init", help="Initialize lockfile with basic defaults")
    init_parser.add_argument("--file", help="Lockfile path (default: llmring.lock)")
    init_parser.add_argument("--force", action="store_true", help="Overwrite existing file")

    # lock validate
    lock_subparsers.add_parser("validate", help="Validate lockfile against registry")

    # lock bump-registry
    lock_subparsers.add_parser("bump-registry", help="Update registry versions")

    # lock chat - conversational lockfile management
    chat_parser = lock_subparsers.add_parser(
        "chat", help="Conversational lockfile management with natural language"
    )
    chat_parser.add_argument(
        "--server-url", help="URL of lockfile MCP server (default: starts embedded server)"
    )
    chat_parser.add_argument(
        "--model",
        default="advisor",
        help="LLM model to use for conversation (default: advisor for intelligent recommendations)",
    )

    # Bind command
    bind_parser = subparsers.add_parser(
        "bind", help="Bind an alias to model(s) with fallback support"
    )
    bind_parser.add_argument("alias", help="Alias name")
    bind_parser.add_argument(
        "model",
        help="Model reference(s) - single or comma-separated for fallbacks (e.g., 'anthropic:claude-3-opus,openai:gpt-4')",
    )
    bind_parser.add_argument("--profile", help="Profile to use (default: default)")

    # Aliases command
    aliases_parser = subparsers.add_parser("aliases", help="List aliases from lockfile")
    aliases_parser.add_argument("--profile", help="Profile to use")

    # List models command
    list_parser = subparsers.add_parser("list", help="List available models")
    list_parser.add_argument(
        "--provider", help="Filter by provider (openai, anthropic, google, ollama)"
    )

    # Chat command
    chat_parser = subparsers.add_parser("chat", help="Send a chat message")
    chat_parser.add_argument("message", help="Message to send")
    chat_parser.add_argument(
        "--model",
        default="fast",
        help="Model alias (fast, balanced, deep) or provider:model",
    )
    chat_parser.add_argument("--system", help="System prompt")
    chat_parser.add_argument("--temperature", type=float, default=0.7, help="Temperature (0.0-2.0)")
    chat_parser.add_argument("--max-tokens", type=int, help="Maximum tokens to generate")
    chat_parser.add_argument("--json", action="store_true", help="Output as JSON")
    chat_parser.add_argument("--verbose", action="store_true", help="Show additional information")
    chat_parser.add_argument("--profile", help="Profile to use for alias resolution")
    chat_parser.add_argument("--stream", action="store_true", help="Stream response in real-time")

    # Info command
    info_parser = subparsers.add_parser("info", help="Show model information")
    info_parser.add_argument(
        "model",
        help="Model alias (fast, balanced, deep) or provider:model (e.g., openai:gpt-4)",
    )
    info_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Providers command
    providers_parser = subparsers.add_parser("providers", help="List configured providers")
    providers_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Push/pull commands removed per source-of-truth v3.8
    # Aliases are managed entirely locally in each codebase's lockfile

    # Stats command
    stats_parser = subparsers.add_parser("stats", help="Show usage statistics")
    stats_parser.add_argument("--verbose", action="store_true", help="Show detailed statistics")
    stats_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Export command
    export_parser = subparsers.add_parser("export", help="Export data to file")
    export_parser.add_argument("--output", help="Output file")
    export_parser.add_argument(
        "--format", choices=["json", "csv"], default="json", help="Export format"
    )

    # Cache management commands
    cache_parser = subparsers.add_parser("cache", help="Registry cache management")
    cache_subparsers = cache_parser.add_subparsers(dest="cache_command", help="Cache commands")
    cache_subparsers.add_parser("clear", help="Clear the registry cache")
    cache_subparsers.add_parser("info", help="Show cache information")

    # Server management commands
    server_parser = subparsers.add_parser("server", help="Manage llmring-server configuration")
    server_subparsers = server_parser.add_subparsers(dest="server_command", help="Server commands")

    server_init_parser = server_subparsers.add_parser("init", help="Create local server config")
    server_init_parser.add_argument(
        "--server", help="Server URL (default: env or http://localhost:8000)"
    )
    server_init_parser.add_argument("--api-key", help="API key to write (default: generate new)")
    server_init_parser.add_argument(
        "--env-file", default=".env.llmring", help="Env file to write (default: ./.env.llmring)"
    )
    server_init_parser.add_argument(
        "--force", action="store_true", help="Overwrite env file if it exists"
    )
    server_init_parser.add_argument(
        "--skip-health-check",
        action="store_true",
        dest="skip_health",
        help="Skip server health validation (useful if the server is not running yet)",
    )

    server_status_parser = server_subparsers.add_parser(
        "status", help="Show server configuration and health"
    )
    server_status_parser.add_argument(
        "--server", help="Override server URL (default: detect from env)"
    )
    server_status_parser.add_argument(
        "--api-key", help="Override API key (default: detect from env)"
    )
    server_status_parser.add_argument(
        "--env-file", default=".env.llmring", help="Env file to inspect (default: ./.env.llmring)"
    )

    server_stats_parser = server_subparsers.add_parser(
        "stats", help="Show aggregated usage statistics"
    )
    server_stats_parser.add_argument("--server", help="Override server URL")
    server_stats_parser.add_argument("--api-key", help="Override API key")
    server_stats_parser.add_argument("--start-date", help="Start date (ISO 8601)")
    server_stats_parser.add_argument("--end-date", help="End date (ISO 8601)")
    server_stats_parser.add_argument(
        "--group-by", choices=["day", "week", "month"], default="day", help="Grouping period"
    )
    server_stats_parser.add_argument("--json", action="store_true", help="Output as JSON")

    server_logs_parser = server_subparsers.add_parser(
        "logs", help="List raw usage logs for the current API key"
    )
    server_logs_parser.add_argument("--server", help="Override server URL")
    server_logs_parser.add_argument("--api-key", help="Override API key")
    server_logs_parser.add_argument(
        "--limit", type=int, default=50, help="Number of rows to return"
    )
    server_logs_parser.add_argument("--offset", type=int, default=0, help="Offset for pagination")
    server_logs_parser.add_argument("--alias", help="Filter by alias")
    server_logs_parser.add_argument("--model", help="Filter by model")
    server_logs_parser.add_argument("--origin", help="Filter by origin")
    server_logs_parser.add_argument("--start-date", help="Start date (ISO 8601)")
    server_logs_parser.add_argument("--end-date", help="End date (ISO 8601)")
    server_logs_parser.add_argument(
        "--output",
        choices=["table", "json", "csv"],
        default="table",
        help="Output format (table is default)",
    )
    server_logs_parser.add_argument("--output-file", help="Write JSON/CSV output to file")
    server_logs_parser.add_argument("--json", action="store_true", help="Alias for --output json")

    server_convo_parser = server_subparsers.add_parser(
        "conversations", help="Inspect stored conversations"
    )
    server_convo_parser.add_argument("--server", help="Override server URL")
    server_convo_parser.add_argument("--api-key", help="Override API key")
    server_convo_parser.add_argument("--limit", type=int, default=20, help="Max conversations")
    server_convo_parser.add_argument("--offset", type=int, default=0, help="Pagination offset")
    server_convo_parser.add_argument("--conversation-id", help="Conversation UUID to fetch")
    server_convo_parser.add_argument(
        "--messages",
        action="store_true",
        help="Include messages when fetching a single conversation",
    )
    server_convo_parser.add_argument("--json", action="store_true", help="Output as JSON")

    server_key_parser = server_subparsers.add_parser("key", help="API key utilities")
    server_key_sub = server_key_parser.add_subparsers(
        dest="server_key_command", help="Server key commands"
    )

    server_key_create = server_key_sub.add_parser("create", help="Generate a new API key")
    server_key_create.add_argument(
        "--env-file", default=".env.llmring", help="Env file to update (default: ./.env.llmring)"
    )
    server_key_create.add_argument(
        "--update-env",
        action="store_true",
        help="Write the generated key to the env file (defaults to print-only)",
    )
    server_key_create.add_argument(
        "--server", help="Server URL to pair with the key when updating the env file"
    )

    server_key_list = server_key_sub.add_parser("list", help="Show current API key configuration")
    server_key_list.add_argument(
        "--env-file", default=".env.llmring", help="Env file to inspect (default: ./.env.llmring)"
    )

    server_key_rotate = server_key_sub.add_parser(
        "rotate", help="Generate a new key and overwrite the env file"
    )
    server_key_rotate.add_argument(
        "--env-file", default=".env.llmring", help="Env file to update (default: ./.env.llmring)"
    )
    server_key_rotate.add_argument(
        "--server", help="Server URL to write (default: detect from env file or environment)"
    )

    # MCP management commands (servers, tools)
    mcp_parser = subparsers.add_parser(
        "mcp", help="Manage MCP servers, tools, and discovery (requires backend)"
    )
    mcp_subparsers = mcp_parser.add_subparsers(dest="mcp_command", help="MCP commands")

    # mcp servers list
    mcp_servers_parser = mcp_subparsers.add_parser(
        "servers", help="Manage MCP servers (register/list)"
    )
    mcp_servers_sub = mcp_servers_parser.add_subparsers(
        dest="mcp_servers_command", help="MCP server subcommands"
    )
    mcp_servers_list = mcp_servers_sub.add_parser("list", help="List MCP servers")
    mcp_servers_list.add_argument(
        "--all", action="store_true", help="Show inactive servers as well"
    )

    # mcp servers register
    mcp_servers_register = mcp_servers_sub.add_parser("register", help="Register a new MCP server")
    mcp_servers_register.add_argument("--name", required=True, help="Server name")
    mcp_servers_register.add_argument("--url", required=True, help="Server URL")
    mcp_servers_register.add_argument(
        "--transport",
        choices=["stdio", "http", "websocket"],
        default="http",
        help="Transport type",
    )

    # mcp tools list
    mcp_tools_parser = mcp_subparsers.add_parser(
        "tools", help="List MCP tools (optionally filtered by server)"
    )
    mcp_tools_parser.add_argument("--server-id", help="Filter by MCP server UUID")

    # mcp prompts list
    mcp_prompts_parser = mcp_subparsers.add_parser(
        "prompts", help="List MCP prompts (optionally filtered by server)"
    )
    mcp_prompts_parser.add_argument("--server-id", help="Filter by MCP server UUID")
    mcp_prompts_sub = mcp_prompts_parser.add_subparsers(
        dest="mcp_prompts_command", help="MCP prompts subcommands"
    )
    mcp_prompts_register = mcp_prompts_sub.add_parser(
        "register", help="Register (append) a prompt to a server"
    )
    mcp_prompts_register.add_argument("--server-id", required=True, help="MCP server UUID")
    mcp_prompts_register.add_argument("--name", required=True, help="Prompt name")
    mcp_prompts_register.add_argument("--description", help="Prompt description")
    mcp_prompts_register.add_argument(
        "--arguments",
        help="JSON object or path to JSON file describing prompt arguments",
    )

    # mcp resources list
    mcp_resources_parser = mcp_subparsers.add_parser(
        "resources", help="List MCP resources (optionally filtered by server)"
    )
    mcp_resources_parser.add_argument("--server-id", help="Filter by MCP server UUID")
    mcp_resources_sub = mcp_resources_parser.add_subparsers(
        dest="mcp_resources_command", help="MCP resources subcommands"
    )
    mcp_resources_register = mcp_resources_sub.add_parser(
        "register", help="Register (append) a resource to a server"
    )
    mcp_resources_register.add_argument("--server-id", required=True, help="MCP server UUID")
    mcp_resources_register.add_argument("--uri", required=True, help="Resource URI")
    mcp_resources_register.add_argument("--name", help="Resource name")
    mcp_resources_register.add_argument("--description", help="Resource description")
    mcp_resources_register.add_argument("--mime-type", help="MIME type")

    # mcp apply -f manifest.json
    mcp_apply_parser = mcp_subparsers.add_parser(
        "apply", help="Apply MCP configuration from a JSON manifest"
    )
    mcp_apply_parser.add_argument("-f", "--file", required=True, help="Path to manifest JSON file")

    # Register command
    register_parser = subparsers.add_parser(
        "register", help="Register with LLMRing server (for SaaS features)"
    )
    register_parser.add_argument("--email", help="Email address for registration")
    register_parser.add_argument("--org", help="Organization name")
    register_parser.add_argument(
        "--project-name",
        help="Optional name for the initial project (default: 'Default Project')",
    )
    register_parser.add_argument(
        "--server",
        help="LLMRing API base URL (defaults to $LLMRING_SERVER_URL or https://api.llmring.ai)",
    )
    register_parser.add_argument(
        "--accept-terms",
        action="store_true",
        help="Accept the Terms of Service without interactive confirmation",
    )
    register_parser.add_argument(
        "--non-interactive",
        action="store_true",
        help="Fail instead of prompting when required fields are missing",
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 1

    # Handle lock subcommands
    if args.command == "lock":
        if not args.lock_command:
            lock_parser.print_help()
            return 1

        lock_commands = {
            "init": cmd_lock_init,
            "validate": cmd_lock_validate,
            "bump-registry": cmd_lock_bump_registry,
            "chat": cmd_lock_chat,
        }

        if args.lock_command in lock_commands:
            return asyncio.run(lock_commands[args.lock_command](args))

    # Handle cache subcommands
    if args.command == "cache":
        if not args.cache_command:
            cache_parser.print_help()
            return 1

        cache_commands = {
            "clear": cmd_cache_clear,
            "info": cmd_cache_info,
        }

        if args.cache_command in cache_commands:
            return asyncio.run(cache_commands[args.cache_command](args))

    # Handle server subcommands
    if args.command == "server":
        if not args.server_command:
            server_parser.print_help()
            return 1

        if args.server_command == "init":
            return asyncio.run(cmd_server_init(args))
        if args.server_command == "status":
            return asyncio.run(cmd_server_status(args))
        if args.server_command == "stats":
            return asyncio.run(cmd_server_stats(args))
        if args.server_command == "logs":
            return asyncio.run(cmd_server_logs(args))
        if args.server_command == "conversations":
            return asyncio.run(cmd_server_conversations(args))
        if args.server_command == "key":
            if not args.server_key_command:
                server_key_parser.print_help()
                return 1
            key_commands = {
                "create": cmd_server_key_create,
                "list": cmd_server_key_list,
                "rotate": cmd_server_key_rotate,
            }
            if args.server_key_command in key_commands:
                return key_commands[args.server_key_command](args)
            server_key_parser.print_help()
            return 1

    # Handle mcp subcommands
    if args.command == "mcp":
        if not args.mcp_command:
            mcp_parser.print_help()
            return 1

        # Nested: mcp servers
        if args.mcp_command == "servers":
            if not args.mcp_servers_command:
                mcp_servers_parser.print_help()
                return 1

            mcp_servers_commands = {
                "list": cmd_mcp_servers_list,
                "register": cmd_mcp_servers_register,
            }
            if args.mcp_servers_command in mcp_servers_commands:
                return asyncio.run(mcp_servers_commands[args.mcp_servers_command](args))

        # mcp tools
        if args.mcp_command == "tools":
            return asyncio.run(cmd_mcp_tools_list(args))

        # mcp prompts
        if args.mcp_command == "prompts":
            if getattr(args, "mcp_prompts_command", None) is None:
                return asyncio.run(cmd_mcp_prompts_list(args))
            prompts_commands = {
                "register": cmd_mcp_prompts_register,
            }
            if args.mcp_prompts_command in prompts_commands:
                return asyncio.run(prompts_commands[args.mcp_prompts_command](args))

        # mcp resources
        if args.mcp_command == "resources":
            if getattr(args, "mcp_resources_command", None) is None:
                return asyncio.run(cmd_mcp_resources_list(args))
            resources_commands = {
                "register": cmd_mcp_resources_register,
            }
            if args.mcp_resources_command in resources_commands:
                return asyncio.run(resources_commands[args.mcp_resources_command](args))

        # mcp apply
        if args.mcp_command == "apply":
            return asyncio.run(cmd_mcp_apply(args))

    # Run the appropriate command
    async_commands = {
        "list": cmd_list_models,
        "chat": cmd_chat,
        "info": cmd_info,
        "stats": cmd_stats,
        "export": cmd_export,
        "register": cmd_register,
    }

    sync_commands = {
        "bind": cmd_bind,
        "aliases": cmd_aliases,
        "providers": cmd_providers,
    }

    if args.command in async_commands:
        return asyncio.run(async_commands[args.command](args))
    elif args.command in sync_commands:
        return sync_commands[args.command](args)
    else:
        parser.print_help()
        return 1


if __name__ == "__main__":
    exit(main())
