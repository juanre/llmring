# File Upload API Implementation Plan

**Status:** IMPLEMENTED ✅
**Version:** llmring v1.4.0+
**Completed:** November 2024
**Priority:** HIGH - Essential for 2025 agent workflows

**Note:** This document reflects the IMPLEMENTED design, not the original plan. The actual implementation differs from the original plan in key ways detailed below.

---

## Executive Summary

Implement unified file upload API across Anthropic, OpenAI, and Google providers to enable:
1. Upload-once, reference-many pattern for efficient file handling
2. Code execution with uploaded datasets (Anthropic)
3. Assistant knowledge bases (OpenAI)
4. Context caching for cost optimization (Google)

**Key Principle:** Abstract provider differences behind clean unified interface while exposing provider-specific capabilities through optional parameters.

---

## Provider Analysis

### Anthropic Files API (Beta)

**Endpoint:** `POST https://api.anthropic.com/v1/files`

**Headers Required:**
```
anthropic-beta: files-api-2025-04-14
```

**Upload Request:**
```python
POST /v1/files
Content-Type: multipart/form-data

file: <binary data>
```

**Response:**
```json
{
  "id": "file-abc123",
  "object": "file",
  "type": "file",
  "size_bytes": 12345,
  "created_at": "2025-11-03T12:00:00Z"
}
```

**Features:**
- Max file size: 500 MB
- Storage limit: 100 GB per organization
- Use with code execution tool
- Reference in messages API with file_id
- Can download files generated by code execution
- Operations: upload, list, retrieve, delete, download (for generated files only)

**Supported Models:** All Claude 3.5+ models

---

### OpenAI Files API

**Endpoint:** `POST https://api.openai.com/v1/files`

**Upload Request:**
```python
POST /v1/files
Content-Type: multipart/form-data

file: <binary data>
purpose: "assistants"  # or "vision", "batch"
```

**Response:**
```json
{
  "id": "file-abc123",
  "object": "file",
  "bytes": 12345,
  "created_at": 1699061776,
  "filename": "data.csv",
  "purpose": "assistants"
}
```

**Features:**
- Purpose-based: assistants, vision, batch
- Use with Assistants API for knowledge bases
- File search tool integration
- Operations: upload, list, retrieve, delete, content (download)

**Limitations:**
- Files uploaded for Assistants are different from chat completions
- Purpose must be specified at upload time

---

### Google Context Caching API

**Different paradigm:** Google doesn't have discrete "file upload" - instead uses context caching for repeated content.

**Create Cache:**
```python
POST https://generativelanguage.googleapis.com/v1beta/cachedContents

{
  "model": "gemini-2.5-flash",
  "contents": [...],  # Content to cache
  "ttl": "3600s"
}
```

**Response:**
```json
{
  "name": "cachedContents/cache-abc123",
  "model": "gemini-2.5-flash",
  "createTime": "2025-11-03T12:00:00Z",
  "expireTime": "2025-11-03T13:00:00Z"
}
```

**Features:**
- Two types: implicit (automatic) and explicit (manual)
- Explicit caching: 90% discount on Gemini 2.5, 75% on Gemini 2.0
- Minimum tokens: 1,024 (2.5 Flash), 4,096 (2.5 Pro)
- Default TTL: 60 minutes
- Use case: Large prompts, repeated context, document analysis

**Mapping Strategy:**
- Treat cache_id as file_id equivalent
- Upload file → convert to content → create cache → return cache_id

---

## Implemented API Design

### ACTUAL Implementation (differs from plan)

**Key Change:** The implemented API uses `model` parameter instead of `purpose` and `provider`.

```python
from typing import Optional, BinaryIO, Union, Any
from pathlib import Path

class LLMRing:
    """Extended with file management capabilities."""

    async def upload_file(
        self,
        file: Union[str, Path, Any],
        model: str,
        ttl_seconds: int = 3600,
        filename: Optional[str] = None,
        **kwargs
    ) -> FileUploadResponse:
        """
        Upload a file and receive a file_id for reuse across requests.

        Args:
            file: File path or file-like object
            model: Model alias or provider:model reference
                - Alias: "summarizer" (provider-agnostic, recommended)
                - Direct: "anthropic:claude-3-5-haiku-20241022"
                Provider is auto-detected from the model parameter
            ttl_seconds: Cache TTL for Google (default: 3600)
            filename: Override filename for upload
            **kwargs: Provider-specific parameters

        Returns:
            FileUploadResponse with file_id and metadata

        Raises:
            FileSizeError: If file exceeds provider limits
            InvalidFileFormatError: If file type not supported
            FileAccessError: Cannot read file

        Example:
            # With alias (provider-agnostic)
            file_resp = await service.upload_file(
                "data.csv",
                model="summarizer"
            )

            # With direct model reference
            file_resp = await service.upload_file(
                "data.csv",
                model="anthropic:claude-3-5-haiku-20241022"
            )

            # Use in request
            request = LLMRequest(
                model="anthropic:claude-sonnet-4-5",
                messages=[Message(role="user", content="Analyze this data")],
                files=[file_resp.file_id],
                tools=[{"type": "code_execution"}]
            )
            response = await service.chat(request)
        """
        ...

    async def list_files(
        self,
        provider: Optional[str] = None,
        purpose: Optional[str] = None,
        limit: int = 100
    ) -> List[FileMetadata]:
        """List uploaded files."""
        ...

    async def get_file(
        self,
        file_id: str,
        provider: Optional[str] = None
    ) -> FileMetadata:
        """Get file metadata."""
        ...

    async def download_file(
        self,
        file_id: str,
        output_path: Union[str, Path],
        provider: Optional[str] = None
    ) -> Path:
        """
        Download a file (only for generated files in Anthropic).

        Note: Can only download files generated by code execution tool,
        not files you uploaded.
        """
        ...

    async def delete_file(
        self,
        file_id: str,
        provider: Optional[str] = None
    ) -> bool:
        """Delete an uploaded file."""
        ...
```

### Schema Extensions

```python
# schemas.py

class FileUploadResponse(BaseModel):
    """Response from file upload."""
    file_id: str
    provider: str
    filename: Optional[str] = None
    size_bytes: int
    created_at: datetime
    purpose: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

class FileMetadata(BaseModel):
    """File metadata."""
    file_id: str
    provider: str
    filename: Optional[str] = None
    size_bytes: int
    created_at: datetime
    purpose: str
    status: Literal["uploaded", "processing", "ready", "error", "expired"]
    metadata: Dict[str, Any] = Field(default_factory=dict)

class LLMRequest(BaseModel):
    """Extended to support file references."""
    messages: List[Message]
    model: Optional[str] = None
    # ... existing fields ...

    # NEW: File references
    files: Optional[List[str]] = None  # List of file_ids

    # ... rest of fields ...
```

---

## Implementation Summary

The file upload API was successfully implemented with the following key differences from the original plan:

### What Changed

1. **API Design:** Used `model` parameter instead of `purpose` + `provider`
   - **Reason:** More consistent with LLMRing's existing API patterns
   - **Benefit:** Provider is automatically detected from model, eliminating redundancy
   - **Impact:** Simpler, more intuitive API surface

2. **Default TTL:** Set to 3600 seconds (1 hour) instead of making it optional
   - **Reason:** Provide sensible default for Google caching
   - **Benefit:** Users don't need to specify TTL unless they want custom behavior

3. **Provider Detection:** Automatic based on model parameter
   - **Reason:** Eliminates need for explicit provider parameter
   - **Benefit:** Less verbose API, fewer parameters to remember

### What Stayed the Same

1. **Core Functionality:** Upload-once-reference-many pattern ✅
2. **Provider Support:** Anthropic, OpenAI, Google all supported ✅
3. **File Management:** list_files(), get_file(), delete_file() all implemented ✅
4. **Error Handling:** FileSizeError, InvalidFileFormatError, FileAccessError ✅
5. **Documentation:** Complete docs in README.md, docs/file-uploads.md, website ✅

---

## Original Implementation Plan (TDD)

### Phase 1: API Design & Schemas (Week 1)

**Tasks:**
1. ✅ Design unified interface (above)
2. Add schemas to `schemas.py`:
   - `FileUploadResponse`
   - `FileMetadata`
   - Extend `LLMRequest` with `files` field
3. Use existing exceptions from `exceptions.py`:
   - `FileSizeError` (already exists, enhanced with provider parameter)
   - `InvalidFileFormatError` (already exists)
   - `FileAccessError` (already exists)
4. Write failing integration tests

**Deliverable:** API surface defined, tests fail with NotImplementedError

---

### Phase 2: Anthropic Files API (Week 2)

**TDD Cycle:**

1. **Write failing test:**
```python
# tests/test_anthropic_files.py

import pytest
from llmring import LLMRing, LLMRequest, Message

@pytest.mark.asyncio
async def test_anthropic_file_upload():
    """Test file upload and reference with Anthropic."""
    async with LLMRing() as service:
        # Upload file
        file_resp = await service.upload_file(
            "tests/fixtures/sample_data.csv",
            purpose="code_execution",
            provider="anthropic"
        )

        assert file_resp.file_id.startswith("file-")
        assert file_resp.provider == "anthropic"
        assert file_resp.size_bytes > 0

        # Use in request
        request = LLMRequest(
            model="anthropic:claude-sonnet-4-5-20250929",
            messages=[Message(
                role="user",
                content="Analyze this CSV and tell me the row count"
            )],
            files=[file_resp.file_id],
            tools=[{"type": "code_execution"}]
        )

        response = await service.chat(request)
        assert response.content

        # Cleanup
        deleted = await service.delete_file(file_resp.file_id)
        assert deleted

@pytest.mark.asyncio
async def test_anthropic_list_files():
    """Test listing files."""
    async with LLMRing() as service:
        # Upload test file
        file_resp = await service.upload_file(
            "tests/fixtures/sample.txt",
            purpose="analysis",
            provider="anthropic"
        )

        # List files
        files = await service.list_files(provider="anthropic")
        assert len(files) > 0
        assert any(f.file_id == file_resp.file_id for f in files)

        # Cleanup
        await service.delete_file(file_resp.file_id)
```

2. **Implement in `anthropic_api.py`:**
```python
# providers/anthropic_api.py

class AnthropicProvider(BaseLLMProvider):

    async def upload_file(
        self,
        file: Union[str, Path, BinaryIO],
        purpose: str = "analysis",
        filename: Optional[str] = None,
        **kwargs
    ) -> FileUploadResponse:
        """Upload file to Anthropic Files API."""
        # Implementation
        ...

    async def list_files(self, purpose: Optional[str] = None, limit: int = 100):
        """List uploaded files."""
        ...

    async def delete_file(self, file_id: str) -> bool:
        """Delete uploaded file."""
        ...
```

3. **Run test** → should pass
4. **Refactor** if needed

**Deliverable:** Anthropic file upload fully working

---

### Phase 3: OpenAI Files API (Week 3)

**TDD Cycle:**

1. **Write failing test:**
```python
# tests/test_openai_files.py

@pytest.mark.asyncio
async def test_openai_file_upload():
    """Test file upload with OpenAI."""
    async with LLMRing() as service:
        file_resp = await service.upload_file(
            "tests/fixtures/knowledge_base.txt",
            purpose="assistant",
            provider="openai"
        )

        assert file_resp.file_id.startswith("file-")
        assert file_resp.provider == "openai"

        # Note: Using files with OpenAI requires Assistants API
        # which is separate from chat completions
        # We'll document this limitation
```

2. **Implement in `openai_api.py`:**
```python
# providers/openai_api.py

class OpenAIProvider(BaseLLMProvider):

    async def upload_file(
        self,
        file: Union[str, Path, BinaryIO],
        purpose: str = "analysis",
        filename: Optional[str] = None,
        **kwargs
    ) -> FileUploadResponse:
        """Upload file to OpenAI Files API."""
        # Map purpose to OpenAI's purpose
        openai_purpose = {
            "assistant": "assistants",
            "code_execution": "assistants",
            "analysis": "assistants"
        }.get(purpose, "assistants")

        # Implementation
        ...
```

3. **Run test** → should pass

**Deliverable:** OpenAI file upload working

---

### Phase 4: Google Context Caching (Week 4)

**TDD Cycle:**

1. **Write failing test:**
```python
# tests/test_google_caching.py

@pytest.mark.asyncio
async def test_google_context_cache():
    """Test context caching with Google (as file upload equivalent)."""
    async with LLMRing() as service:
        # Upload creates a cache
        file_resp = await service.upload_file(
            "tests/fixtures/large_document.txt",
            purpose="cache",
            provider="google",
            ttl_seconds=3600
        )

        assert file_resp.file_id.startswith("cachedContents/")
        assert file_resp.provider == "google"

        # Use cached content in request
        request = LLMRequest(
            model="google:gemini-2.5-flash",
            messages=[Message(
                role="user",
                content="Summarize the cached document"
            )],
            files=[file_resp.file_id]  # References cache
        )

        response = await service.chat(request)
        assert response.content
```

2. **Implement in `google_api.py`:**
```python
# providers/google_api.py

class GoogleProvider(BaseLLMProvider):

    async def upload_file(
        self,
        file: Union[str, Path, BinaryIO],
        purpose: str = "cache",
        ttl_seconds: int = 3600,
        filename: Optional[str] = None,
        **kwargs
    ) -> FileUploadResponse:
        """
        Create context cache from file (Google's file upload equivalent).

        Note: Google doesn't have discrete file uploads like Anthropic/OpenAI.
        Instead, we create a context cache from the file content.
        """
        # Read file content
        if isinstance(file, (str, Path)):
            with open(file, 'r') as f:
                content = f.read()
        else:
            content = file.read()

        # Create cache
        cache_resp = await self.client.caching.create(
            model=self.default_model,
            contents=[{"parts": [{"text": content}]}],
            ttl=f"{ttl_seconds}s"
        )

        return FileUploadResponse(
            file_id=cache_resp.name,  # e.g., "cachedContents/abc123"
            provider="google",
            filename=filename,
            size_bytes=len(content.encode()),
            created_at=...,
            purpose="cache",
            metadata={"ttl_seconds": ttl_seconds}
        )
```

3. **Run test** → should pass

**Deliverable:** Google caching as file upload working

---

### Phase 5: Integration & Service Layer (Week 5)

**Implement in `service.py`:**

```python
# service.py

class LLMRing:

    async def upload_file(
        self,
        file: Union[str, Path, BinaryIO],
        purpose: str = "analysis",
        provider: Optional[str] = None,
        **kwargs
    ) -> FileUploadResponse:
        """
        Upload file to provider and return file_id.

        Routes to appropriate provider based on:
        1. Explicit provider parameter
        2. Current default model's provider
        3. Purpose-based selection
        """
        # Determine provider
        if provider:
            target_provider = provider
        else:
            # Auto-detect from current model or purpose
            target_provider = self._select_provider_for_upload(purpose)

        # Get provider instance
        provider_obj = self.get_provider(target_provider)

        # Upload
        response = await provider_obj.upload_file(
            file=file,
            purpose=purpose,
            **kwargs
        )

        return response

    async def chat(self, request: LLMRequest) -> LLMResponse:
        """
        Extended to handle file references.
        """
        # ... existing code ...

        # Handle files if present
        if request.files:
            # Provider-specific file handling
            if provider_name == "anthropic":
                # Add files to message content
                ...
            elif provider_name == "openai":
                # Files must be used with Assistants API
                # Document this limitation
                ...
            elif provider_name == "google":
                # Use cached content
                ...

        # ... rest of implementation ...
```

**Write integration tests:**

```python
# tests/test_file_integration.py

@pytest.mark.asyncio
async def test_cross_provider_file_usage():
    """Test that file_id is provider-bound."""
    async with LLMRing() as service:
        # Upload to Anthropic
        anthropic_file = await service.upload_file(
            "test.csv",
            purpose="code_execution",
            provider="anthropic"
        )

        # Try to use with OpenAI model (should fail gracefully)
        request = LLMRequest(
            model="openai:gpt-4o",
            messages=[Message(role="user", content="Analyze")],
            files=[anthropic_file.file_id]
        )

        with pytest.raises(ValueError, match="file_id from anthropic"):
            await service.chat(request)
```

**Deliverable:** Full integration working, proper error handling

---

### Phase 6: Documentation (Week 6)

#### A. Technical Documentation

Create `docs/file-uploads.md`:

```markdown
# File Uploads

## Overview

Upload files once, reference many times across requests for efficient file handling.

## Quick Start

```python
from llmring import LLMRing, LLMRequest, Message

async with LLMRing() as service:
    # Upload file
    file_resp = await service.upload_file(
        "data.csv",
        purpose="code_execution"
    )

    # Use in multiple requests
    request = LLMRequest(
        model="anthropic:claude-sonnet-4-5",
        messages=[Message(role="user", content="Analyze")],
        files=[file_resp.file_id],
        tools=[{"type": "code_execution"}]
    )

    response = await service.chat(request)
```

## Provider Comparison

| Feature | Anthropic | OpenAI | Google |
|---------|-----------|--------|--------|
| Upload | ✅ Files API | ✅ Files API | ⚠️ Context Caching |
| Reference | file_id | file_id | cache_id |
| Use Case | Code execution, general | Assistants | Cost optimization |
| Max Size | 500 MB | varies | N/A |

... (detailed docs)
```

#### B. README Updates

Add to `README.md`:

```markdown
## File Uploads

Upload files once and reference them across multiple requests:

```python
# Upload dataset
file_resp = await service.upload_file("data.csv", purpose="code_execution")

# Use with code execution
request = LLMRequest(
    model="anthropic:claude-sonnet-4-5",
    messages=[Message(role="user", content="Analyze this data")],
    files=[file_resp.file_id],
    tools=[{"type": "code_execution"}]
)
response = await service.chat(request)
```

See [File Uploads Documentation](docs/file-uploads.md) for details.
```

#### C. Website Documentation

Create `llmring.ai/content/docs/file-uploads.md`:

```markdown
+++
title = "File Uploads"
weight = 40
+++

# File Uploads

Upload files to providers and reference them efficiently across requests.

## Why File Uploads?

- **Efficiency**: Upload once, use many times
- **Cost**: Reduce token usage for large files
- **Agents**: Essential for code execution workflows
- **Performance**: Faster than embedding in messages

... (marketing-focused docs with examples)
```

**Deliverable:** Complete documentation

---

## Gap Documentation Plan

### 1. README.md Section

Add new section after "Advanced Features":

```markdown
## Current Limitations & Workarounds

LLMRing provides a unified interface for core LLM functionality across providers. Some advanced provider-specific features require workarounds or direct SDK access.

### ✅ Fully Supported

- Chat completions
- Streaming (SSE)
- Tool calling
- Structured output (JSON schema)
- Vision and multimodal
- Documents (PDFs, images)
- File uploads (v1.4.0+)

### ⚠️ Supported via Workarounds

#### Provider-Specific Parameters

Use `extra_params` for provider-specific features:

```python
# OpenAI logprobs
request = LLMRequest(
    model="openai:gpt-4o",
    messages=[...],
    extra_params={
        "logprobs": True,
        "top_logprobs": 5,
        "seed": 12345
    }
)

# Google safety settings
request = LLMRequest(
    model="google:gemini-2.5-flash",
    messages=[...],
    extra_params={
        "safety_settings": [{
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        }]
    }
)

# Anthropic thinking budget
request = LLMRequest(
    model="anthropic:claude-sonnet-4-5",
    messages=[...],
    extra_params={
        "thinking": {
            "type": "enabled",
            "budget_tokens": 5000
        }
    }
)
```

See [Provider-Specific Features](docs/provider-specific-features.md) for complete list.

#### Direct SDK Access

For features not exposed by llmring, access the raw SDK client:

```python
# Get underlying client
anthropic_client = service.get_provider("anthropic").client
openai_client = service.get_provider("openai").client
google_client = service.get_provider("google").client

# Use provider SDK directly
response = await anthropic_client.messages.create(
    model="claude-sonnet-4-5-20250929",
    messages=[...],
    # Any Anthropic SDK parameter
)
```

### ❌ Not Yet Supported

#### Real-time Audio/Video Streaming

OpenAI Realtime API and Google Live API provide WebSocket-based real-time streaming for voice/video applications. This is planned for a future release.

**Workaround:** Use provider SDK directly:

```python
from openai import AsyncOpenAI

client = AsyncOpenAI()
async with client.beta.realtime.connect(model="gpt-4o-realtime") as session:
    await session.send_audio(audio_data)
    async for event in session:
        # Handle real-time events
        ...
```

#### OpenAI Agents SDK

OpenAI's Agents SDK is a separate framework (like LangChain) built on top of the API. Use it directly with llmring:

```python
from openai_agents import Agent
from llmring import LLMRing

service = LLMRing()
openai_client = service.get_provider("openai").client

# Use Agents SDK with llmring's OpenAI client
agent = Agent(
    name="assistant",
    client=openai_client,
    instructions="You are a helpful assistant"
)
```

See [Advanced Patterns](docs/advanced-patterns.md) for integration examples.
```

### 2. Website Documentation Page

Create `llmring.ai/content/docs/limitations.md`:

```markdown
+++
title = "Limitations & Workarounds"
description = "Understanding llmring's scope and how to access advanced features"
weight = 90
+++

# Understanding llmring's Scope

llmring provides a **unified interface for core LLM functionality**. It excels at making common tasks consistent across providers while providing escape hatches for advanced features.

## Design Philosophy

### What llmring Unifies

We unify features that:
1. **Work across all providers** - Chat, streaming, tools, structured output
2. **Have similar semantics** - File uploads, multimodal input
3. **Add clear value** - Simplified API surface, multi-provider fallback

### What llmring Doesn't Unify

We don't try to unify:
1. **Provider-unique features** - Use provider SDK directly
2. **Experimental APIs** - Too unstable to abstract
3. **Framework-level concerns** - Agent orchestration, chains

## Feature Matrix

| Feature | Support Level | Approach |
|---------|--------------|----------|
| Chat completions | ✅ First-class | Unified API |
| Streaming | ✅ First-class | Unified API |
| Tools | ✅ First-class | Unified API |
| Structured output | ✅ First-class | Unified API |
| Multimodal | ✅ First-class | Unified API |
| File uploads | ✅ First-class | Unified API (v1.4.0+) |
| Provider params | ⚠️ Workaround | `extra_params` |
| Logprobs | ⚠️ Workaround | `extra_params` |
| Safety settings | ⚠️ Workaround | `extra_params` |
| Real-time audio | ❌ Use SDK | Direct SDK access |
| Agents frameworks | ❌ Use SDK | Direct SDK access |

## When to Use What

### Use llmring's Unified API When:

- Building multi-provider applications
- Implementing provider fallback
- Optimizing costs across providers
- Prototyping and MVPs
- Standard chat/completion workflows

### Use extra_params When:

- Need provider-specific tuning
- Accessing well-documented provider features
- One-off provider-specific requirements

### Use Direct SDK Access When:

- Experimental/beta features
- Real-time audio/video
- Provider-specific frameworks
- Complex provider-specific workflows

## Examples

[... detailed examples for each approach ...]

## See Also

- [Provider-Specific Features](./provider-specific-features.md)
- [Advanced Patterns](./advanced-patterns.md)
- [API Reference](./api-reference.md)
```

---

## Success Criteria

### Functional Requirements

- [ ] Files can be uploaded to all three providers
- [ ] Files can be referenced in requests via file_id
- [ ] Files can be listed, retrieved, and deleted
- [ ] Provider differences are abstracted appropriately
- [ ] Error handling is comprehensive
- [ ] Tests achieve >90% coverage

### Non-Functional Requirements

- [ ] API is intuitive and consistent with existing llmring patterns
- [ ] Documentation is comprehensive with examples
- [ ] Performance is acceptable (upload <5s for 10MB files)
- [ ] Error messages are clear and actionable

### Documentation Requirements

- [ ] Technical docs in `docs/file-uploads.md`
- [ ] README.md updated with examples
- [ ] Website docs at `llmring.ai/content/docs/file-uploads.md`
- [ ] Gaps documented in README and website
- [ ] Provider comparison table included

---

## Timeline

**Week 1:** API design & schemas
**Week 2:** Anthropic implementation
**Week 3:** OpenAI implementation
**Week 4:** Google implementation
**Week 5:** Integration & testing
**Week 6:** Documentation

**Total:** 6 weeks (~1.5 months)

---

## Risks & Mitigations

### Risk: Provider API Changes

**Mitigation:** All providers have beta headers or versioning. Pin to stable versions.

### Risk: Google's Different Model

**Mitigation:** Accept that Google is different - map caching to file uploads conceptually.

### Risk: OpenAI Assistants Complexity

**Mitigation:** Document limitations clearly. File uploads work, but using them requires Assistants API which is separate from chat.

### Risk: Scope Creep

**Mitigation:** Stick to upload/reference pattern. Don't try to unify Assistants API or complex agent workflows.

---

## Open Questions

1. **Google caching TTL:** Should we have a different default for Google vs hard-coded 3600s?
   - **Decision:** Make it configurable, default to 3600s

2. **OpenAI Assistants:** Should we implement lightweight Assistants wrapper?
   - **Decision:** No - out of scope. Document how to use raw SDK.

3. **File size limits:** Should we enforce limits?
   - **Decision:** Yes - fail fast with clear error messages

4. **Cross-provider file reference:** Should we prevent or allow with warning?
   - **Decision:** Prevent - file_id is provider-bound

---

## Next Steps

1. Review and approve this plan
2. Create GitHub issue with checkboxes for each phase
3. Begin Phase 1: API design & schemas
4. Set up weekly check-ins to track progress
